{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this ETL project i will use pyspark as an ETL tool to extract data from postgres database (company) and make transformation using dataframe operations then load it in DWH (Company_DWH) with star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresTest\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", './postgresql-42.3.9.jar') \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"./postgresql-42.3.9.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we established connection to postgres server\n",
    "jdbc_url = \"jdbc:postgresql://127.0.0.1:5432/company\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"P@ssw0rd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each table in my database i will creat dataframe\n",
    "customers_df = spark.read.jdbc(url=jdbc_url, table=\"public.customers\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------------------+------------+--------------+--------------------------+\n",
      "|customer_id|full_name|email                 |phone       |address       |created_at                |\n",
      "+-----------+---------+----------------------+------------+--------------+--------------------------+\n",
      "|1          |John Doe |john.doe93@example.com|NULL        |123 Elm St.   |2024-12-13 16:07:58.781834|\n",
      "|2          |John Doe |john.doe55@example.com|123-456-4281|123 Elm Street|2024-06-26 16:07:58.781834|\n",
      "|3          |John Doe |john.doe87@example.com|123-456-6859|123 Elm Street|2024-05-03 16:07:58.781834|\n",
      "|4          |John Doe |john.doe36@example.com|123-456-5522|123 Elm St.   |2024-09-06 16:07:58.781834|\n",
      "|5          |John Doe |NULL                  |NULL        |123 Elm Street|2025-02-08 16:07:58.781834|\n",
      "|6          |John Doe |john.doe7@example.com |123-456-6407|123 Elm Street|2024-12-28 16:07:58.781834|\n",
      "|7          |John Doe |NULL                  |NULL        |123 Elm St.   |2024-11-17 16:07:58.781834|\n",
      "|8          |John Doe |john.doe37@example.com|123-456-6250|123 Elm Street|2025-01-29 16:07:58.781834|\n",
      "|9          |john doe |john.doe51@example.com|123-456-5076|123 Elm Street|2025-01-07 16:07:58.781834|\n",
      "|10         |John Doe |NULL                  |123-456-2105|123 Elm Street|2024-10-02 16:07:58.781834|\n",
      "+-----------+---------+----------------------+------------+--------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will see many nulls and dublicates values. We will clean it\n",
    "customers_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.jdbc(url=jdbc_url, table=\"public.orders\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------------------------+----------------------+---------+\n",
      "|order_id|customer_id|order_date                   |amount                |status   |\n",
      "+--------+-----------+-----------------------------+----------------------+---------+\n",
      "|1       |171        |2024-10-02 16:07:58.781834+00|830.190000000000000000|Completed|\n",
      "|2       |876        |2024-09-29 16:07:58.781834+00|743.880000000000000000|Completed|\n",
      "|3       |362        |2024-12-28 16:07:58.781834+00|762.310000000000000000|Completed|\n",
      "|4       |603        |2024-10-17 16:07:58.781834+00|NULL                  |Completed|\n",
      "|5       |578        |2024-12-12 16:07:58.781834+00|375.830000000000000000|Completed|\n",
      "|6       |139        |2024-12-14 16:07:58.781834+00|83.020000000000000000 |completed|\n",
      "|7       |12         |2024-09-30 16:07:58.781834+00|182.670000000000000000|completed|\n",
      "|8       |713        |2024-06-07 16:07:58.781834+00|484.240000000000000000|Completed|\n",
      "|9       |938        |2025-02-08 16:07:58.781834+00|NULL                  |completed|\n",
      "|10      |934        |2025-03-19 16:07:58.781834+00|300.500000000000000000|completed|\n",
      "+--------+-----------+-----------------------------+----------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_df = spark.read.jdbc(url=jdbc_url, table=\"public.payments\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payments_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------------------------+----------------------+-----------+\n",
      "|payment_id|order_id|payment_date                 |amount                |method     |\n",
      "+----------+--------+-----------------------------+----------------------+-----------+\n",
      "|1         |27      |2024-03-30 16:07:58.781834+00|990.000000000000000000|Credit Card|\n",
      "|2         |72      |2024-09-10 16:07:58.781834+00|832.240000000000000000|Credit Card|\n",
      "|3         |100     |2025-02-22 16:07:58.781834+00|NULL                  |Credit Card|\n",
      "|4         |229     |2024-12-09 16:07:58.781834+00|569.350000000000000000|credit card|\n",
      "|5         |982     |2024-04-21 16:07:58.781834+00|210.770000000000000000|credit card|\n",
      "|6         |705     |2024-05-29 16:07:58.781834+00|52.050000000000000000 |Credit Card|\n",
      "|7         |870     |2024-05-04 16:07:58.781834+00|578.620000000000000000|Credit Card|\n",
      "|8         |530     |2024-10-29 16:07:58.781834+00|846.130000000000000000|credit card|\n",
      "|9         |22      |2024-08-18 16:07:58.781834+00|993.000000000000000000|credit card|\n",
      "|10        |643     |2024-12-27 16:07:58.781834+00|706.210000000000000000|Credit Card|\n",
      "+----------+--------+-----------------------------+----------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payments_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets do some cleaning on these dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Dublicates\n",
    "customers_df = customers_df.dropDuplicates([\"email\", \"phone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Handling\n",
    "customers_df = customers_df.fillna({\"email\": \"unknown@example.com\", \"phone\": \"000-000-0000\", \"address\": \"Unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data consistent with same format\n",
    "from pyspark.sql.functions import lower, col\n",
    "customers_df = customers_df.withColumn(\"email\", lower(col(\"email\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix date column\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "orders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.dropna(subset=[\"order_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.withColumn(\"status\", lower(col(\"status\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_df = payments_df.withColumn(\"method\", lower(col(\"method\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will make DWH\n",
    "#### i will create another database to not use the OLTP one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with star schema design\n",
    "#### - Dim_Customer\n",
    "#### - Dim_Date\n",
    "#### - Dim_Payment_Method\n",
    "#### - Fact_Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer = customers_df.select(\"customer_id\", \"full_name\", \"email\", \"phone\", \"address\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = false)\n",
      " |-- phone: string (nullable = false)\n",
      " |-- address: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date = orders_df.select(col(\"order_date\").alias(\"full_date\")) \\\n",
    "    .withColumn(\"year\", year(col(\"full_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"full_date\"))) \\\n",
    "    .withColumn(\"weekday\", date_format(col(\"full_date\"), \"EEEE\")) \\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_payment_method = payments_df.select(col(\"method\").alias(\"payment_method\")).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_method: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_payment_method.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders = orders_df.join(payments_df.select(\"order_id\", \"method\"), [\"order_id\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders = fact_orders.join(dim_customer, [\"customer_id\"], \"left\") \\\n",
    "                         .join(dim_date, fact_orders[\"order_date\"] == dim_date[\"full_date\"], \"left\") \\\n",
    "                         .drop(\"full_date\") \\\n",
    "                         .join(dim_payment_method, fact_orders[\"method\"] == dim_payment_method[\"payment_method\"], \"left\") \\\n",
    "                         .drop(\"method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, monotonically_increasing_id\n",
    "\n",
    "fact_orders = fact_orders.withColumn(\"date_id\", date_format(col(\"order_date\"), \"yyyyMMdd\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "fact_orders = fact_orders.withColumn(\n",
    "    \"method_id\",\n",
    "    when(col(\"payment_method\") == \"Credit Card\", 1)\n",
    "    .when(col(\"payment_method\") == \"PayPal\", 2)\n",
    "    .when(col(\"payment_method\") == \"Bank Transfer\", 3)\n",
    "    .otherwise(0)  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- method_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "fact_orders = fact_orders.withColumn(\"date_id\", \n",
    "    (year(\"order_date\") * 10000) + (month(\"order_date\") * 100) + dayofmonth(\"order_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_orders = fact_orders.select(\"order_id\", \"customer_id\", \"date_id\", \"amount\", \"method_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- method_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(df, table_name):\n",
    "    df.write \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", \"jdbc:postgresql://127.0.0.1:5432/company_DWH\") \\\n",
    "      .option(\"dbtable\", table_name) \\\n",
    "      .option(\"user\", \"postgres\") \\\n",
    "      .option(\"password\", \"P@ssw0rd\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "\n",
    "write_to_postgres(dim_customer, \"Dim_Customer\")\n",
    "write_to_postgres(dim_date, \"Dim_Date\")\n",
    "write_to_postgres(dim_payment_method, \"Dim_Payment_Method\")\n",
    "write_to_postgres(fact_orders, \"Fact_Orders\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
