{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark *INSIGHTS* Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in this project we will use this dataset\n",
    "#### 1- Retailrocket recommender system dataset\n",
    "        -The dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (item_properties.сsv) and a file, which describes category tree (category_tree.сsv). The data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues. The purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/28 16:02:07 WARN Utils: Your hostname, filo resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlp0s20f3)\n",
      "25/03/28 16:02:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/28 16:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pyspark insights\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", './postgresql-42.3.9.jar') \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"./postgresql-42.3.9.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events_df = spark.read.csv(path='./data/events.csv', header=True, inferSchema=True, samplingRatio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- visitorid: integer (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- transactionid: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+------+-------------+\n",
      "|timestamp    |visitorid|event|itemid|transactionid|\n",
      "+-------------+---------+-----+------+-------------+\n",
      "|1433221332117|257597   |view |355908|NULL         |\n",
      "|1433224214164|992329   |view |248676|NULL         |\n",
      "|1433221999827|111016   |view |318965|NULL         |\n",
      "|1433221955914|483717   |view |253185|NULL         |\n",
      "|1433221337106|951259   |view |367447|NULL         |\n",
      "|1433224086234|972639   |view |22556 |NULL         |\n",
      "|1433221923240|810725   |view |443030|NULL         |\n",
      "|1433223291897|794181   |view |439202|NULL         |\n",
      "|1433220899221|824915   |view |428805|NULL         |\n",
      "|1433221204592|339335   |view |82389 |NULL         |\n",
      "+-------------+---------+-----+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tree_df = spark.read.csv(path='./data/category_tree.csv', header=True, inferSchema=True, samplingRatio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- categoryid: integer (nullable = true)\n",
      " |-- parentid: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_tree_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|categoryid|parentid|\n",
      "+----------+--------+\n",
      "|1016      |213     |\n",
      "|809       |169     |\n",
      "|570       |9       |\n",
      "|1691      |885     |\n",
      "|536       |1691    |\n",
      "|231       |NULL    |\n",
      "|542       |378     |\n",
      "|1146      |542     |\n",
      "|1140      |542     |\n",
      "|1479      |1537    |\n",
      "+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_tree_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_properties_df1 = spark.read.csv(\"./data/item_properties_part1.csv\", header=True, inferSchema=True, samplingRatio=0.01)\n",
    "item_properties_df2 = spark.read.csv(\"./data/item_properties_part2.csv\", header=True, inferSchema=True, samplingRatio=0.01)\n",
    "\n",
    "item_properties_df = item_properties_df1.unionByName(item_properties_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- property: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_properties_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+----------+-------------------------------+\n",
      "|timestamp    |itemid|property  |value                          |\n",
      "+-------------+------+----------+-------------------------------+\n",
      "|1435460400000|460429|categoryid|1338                           |\n",
      "|1441508400000|206783|888       |1116713 960601 n277.200        |\n",
      "|1439089200000|395014|400       |n552.000 639502 n720.000 424566|\n",
      "|1431226800000|59481 |790       |n15360.000                     |\n",
      "|1431831600000|156781|917       |828513                         |\n",
      "|1436065200000|285026|available |0                              |\n",
      "|1434250800000|89534 |213       |1121373                        |\n",
      "|1431831600000|264312|6         |319724                         |\n",
      "|1433646000000|229370|202       |1330310                        |\n",
      "|1434250800000|98113 |451       |1141052 n48.000                |\n",
      "+-------------+------+----------+-------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_properties_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets do some cleaning operations and preparartion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "# convert from millisconds to timestamp\n",
    "events_df = events_df.withColumn(\"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\"))\n",
    "item_properties_df = item_properties_df.withColumn(\"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df.fillna({\"transactionid\": -1})\n",
    "\n",
    "events_df = events_df.dropna(subset=[\"itemid\"])\n",
    "item_properties_df = item_properties_df.dropna(subset=[\"itemid\"])\n",
    "category_tree_df = category_tree_df.dropna(subset=[\"categoryid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = events_df.join(item_properties_df, on=\"itemid\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.join(category_tree_df, df_joined[\"value\"] == category_tree_df[\"categoryid\"], how=\"left\") \\\n",
    "    .drop(\"value\")  # Remove the old column as it has been replaced by categoryid from category_tree_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.fillna({\n",
    "    \"parentid\": -1,        \n",
    "    \"property\": \"unknown\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- visitorid: integer (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- transactionid: integer (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- property: string (nullable = false)\n",
      " |-- categoryid: integer (nullable = true)\n",
      " |-- parentid: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- visitorid: integer (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- transactionid: integer (nullable = false)\n",
      " |-- property: string (nullable = false)\n",
      " |-- categoryid: integer (nullable = true)\n",
      " |-- parentid: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_partition = df_joined.repartition(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_partition.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://127.0.0.1:5432/pyspark_insights\") \\\n",
    "  .option(\"dbtable\", \"ecommerce_events\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"password\", \"P@ssw0rd\") \\\n",
    "  .option(\"batchsize\", \"10000\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
